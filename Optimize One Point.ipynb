{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing a single point (n-dimensional pitch aggregate)\n",
    "\n",
    "This noteboook demonstrates how to find the optimal tuning for a single point. In this case, a \"point\" is a pitch aggregate in n-dimensional space, where the pitch aggregate contains n+1 pitches. For example, a triad would be 2-dimensional. The values of the dimensions are equal to the base-2 logarithms of the ratios of the frequencies from one of the pitches (which we might call the \"root\").\n",
    "\n",
    "## TensorBoard\n",
    "\n",
    "If you would like to use [TensorBoard](https://www.tensorflow.org/tensorboard) to dig into the logs, you will need to install and run it from the command line. The following installation example uses conda, but you can use pip if you so desire.\n",
    "\n",
    "```bash\n",
    "conda install -n <your_environment> tensorboard\n",
    "tensorboard --logdir logs/fit\n",
    "```\n",
    "\n",
    "## Setup and Usage\n",
    "\n",
    "First, import all the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "devices = tf.config.experimental.get_visible_devices('GPU')\n",
    "if len(devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(devices[0], True)\n",
    "    \n",
    "import harmonic_distance as hd\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Constants\n",
    "\n",
    "These constants should be scaled appropriately. Comments about each constant are given in-line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"curve\" of the parabola around each possible pitch. A higher \n",
    "# value will lead to fewer possible pitches.\n",
    "C = 0.01\n",
    "\n",
    "# The learning rate of the optimization algorithm. A higher value \n",
    "# will converge more quickly, if possible, but might never converge.\n",
    "LEARNING_RATE = 1.0e-2\n",
    "\n",
    "# The convergence threshold is the norm of the gradients of the loss\n",
    "# function. This is used to test whether a proper \"valley\" has been found.\n",
    "CONVERGENCE_THRESHOLD = 1.0e-5\n",
    "\n",
    "# Maximum number of iterations before giving up on convergence.\n",
    "MAX_ITERS = 1000\n",
    "\n",
    "# These are logarithmic, so we're starting by trying to tune the 12TET major third to 5/4\n",
    "STARTING_VALUES = [4.0 / 12.0]\n",
    "\n",
    "# Dimensionality of the space. This is only finding the minimum of a single interval.\n",
    "DIMENSIONS=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the VectorSpace\n",
    "\n",
    "The _vector space_ can be thought of as the list of \"all possible pitches.\"\n",
    "\n",
    "In this implementation, we're using a `VectorSpace` subclass of [`tf.Module`](https://www.tensorflow.org/api_docs/python/tf/Module?version=stable) to cache the variables that do not change through each iteration.\n",
    "\n",
    "### `hd.vectors.space_graph_altered_permutations`\n",
    "\n",
    "The array `[5, 5, 3, 3, 2, 1]` sets the number of degrees along each dimension of the harmonic lattice (Tenney 19XX) that are available for tuning. The `bounds` value of `(0.0, 4.0)` restricts these harmonic possibilities to a 4-octave range in pitch space.\n",
    "\n",
    "### `hd.tenney.hd_aggregate_graph`\n",
    "\n",
    "Calculates the harmonic distance of every vector.\n",
    "\n",
    "### `vectors_reasonable`\n",
    "\n",
    "Restricts the possible pitches to values with a harmonic distances less than `9.0`. This is useful for the purposes of reducing memory consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamped_writer(var=''):\n",
    "    return tf.summary.create_file_writer('logs/fit/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Minimizer(tf.Module):\n",
    "    def __init__(self, dimensions=1, **kwargs):\n",
    "        self.dimensions = dimensions\n",
    "        self.vs = hd.vectors.VectorSpace(**kwargs)\n",
    "        self.log_pitches = tf.Variable(tf.zeros((dimensions), dtype=tf.float64), dtype=tf.float64)\n",
    "        self.step = tf.Variable(0, dtype=tf.int64)\n",
    "    \n",
    "    def minimize(self):\n",
    "        self.step.assign(0)\n",
    "        self.writers = [timestamped_writer(var='/main')]\n",
    "        for idx in range(self.dimensions):\n",
    "            self.writers.append(timestamped_writer(var=(\"/pitch{}\".format(idx+1))))\n",
    "        # Since Adagrad maintains state, we need to reset it at the start of each call to minimize()\n",
    "        self.opt = tf.optimizers.Adagrad(learning_rate=LEARNING_RATE)\n",
    "        self.write_values()\n",
    "        while self.stopping_op() and self.step < MAX_ITERS:\n",
    "            self.opt.minimize(lambda: self.loss(self.log_pitches), self.log_pitches)\n",
    "            self.step.assign_add(1)\n",
    "            self.write_values()\n",
    "        if self.stopping_op():\n",
    "            tf.print(\"Did not converge\")\n",
    "            with self.writers[0].as_default():\n",
    "                tf.summary.text(\"convergence\", \"did not converge\", step=self.step)\n",
    "        else:\n",
    "            with self.writers[0].as_default():\n",
    "                tf.summary.text(\"convergence\", \"converged\", step=self.step)\n",
    "\n",
    "    def write_values(self):\n",
    "        current_loss = self.loss(self.log_pitches)\n",
    "        with self.writers[0].as_default():\n",
    "            tf.summary.scalar(\"loss\", tf.reduce_mean(current_loss, axis=-1), step=self.step)\n",
    "            with tf.GradientTape() as g:\n",
    "                dz_dv = g.gradient(self.loss(self.log_pitches), self.log_pitches)\n",
    "            norms = tf.nn.l2_loss(dz_dv)\n",
    "            tf.summary.scalar(\"loss-norm\", norms, step=self.step)\n",
    "        for idx, writer in enumerate(self.writers[1:]):\n",
    "            with writer.as_default():\n",
    "                tf.summary.scalar(\"loss\", current_loss[idx], step=self.step)\n",
    "                tf.summary.scalar(\"pitch\", self.log_pitches[idx], step=self.step)\n",
    "\n",
    "    @tf.function\n",
    "    def loss(self, var_list):\n",
    "        return hd.optimize.parabolic_loss_function(self.vs.pds, self.vs.hds, var_list, curves=(C, C))\n",
    "            \n",
    "    @tf.function\n",
    "    def stopping_op(self):\n",
    "        with tf.GradientTape() as g:\n",
    "            dz_dv = g.gradient(self.loss(self.log_pitches), self.log_pitches)\n",
    "        norms = tf.nn.l2_loss(dz_dv)\n",
    "        return norms >= CONVERGENCE_THRESHOLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing one interval\n",
    "\n",
    "When we set `dimensions=1`, we are only attempting to minimize the harmonic distance of a single interval using the Adagrad (Adaptive Gradient Descent) algorithm.\n",
    "\n",
    "## `prime_limits`\n",
    "\n",
    "The `prime_limits` variable sets the maximum number of dimensions along the Tenney \"harmonic lattice\" that are used when generating the possible vector space. The defaults are overwritten here with smaller values (and restricting to a 7-limit harmonic space) to facilitate faster computation.\n",
    "\n",
    "### About the Adagrad optimization algorithm\n",
    "\n",
    "The Adagrad algorithm is short for \"Adaptive Gradient Descent,\" and is implemented as part of the Tensorflow package. The algorithm uses a different learning rate for each dimension in the training variable vector (equivalent to a single \"feature\" in most machine learning applications). Variable that change by a large amount have their learning rates increased, while variables that change very little have very low learning rates. The advantage to this method is that it converges much more quickly than traditional gradient descent.\n",
    "\n",
    "### Other algorithms\n",
    "\n",
    "The Adam (Adagrad with Momentum) algorithm was also tried extensively. The \"momentum\" feature has the advantage for most applications of ensuring that the function does not fall into small local minima; however, for our purpose, we are very interested in finding local minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working with a smaller set of prime limits here for speed of debugging\n",
    "minimizer = Minimizer(dimensions=1, prime_limits=[4, 3, 2, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the algorithm\n",
    "\n",
    "After initializing the `Minimizer`, we can assign the starting pitches as a one-dimensional `Tensor` or `Array` of shape `(dims,)`, where `dims` is equal to the `dimensions` argument from the initialization of the `Minimizer`.\n",
    "\n",
    "Then, a single call to `minimizer.minimize()` will run the loop up to `MAX_ITERS` times until convergence is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(1,) dtype=float64, numpy=array([0.32192849])>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimizer.log_pitches.assign([4/12])\n",
    "minimizer.minimize()\n",
    "minimizer.log_pitches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.0, 4.0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winner = hd.vectors.closest_from_log([minimizer.log_pitches], minimizer.vs.vectors)\n",
    "hd.vectors.to_ratio(winner[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
